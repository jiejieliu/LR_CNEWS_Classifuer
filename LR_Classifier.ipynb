{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['星座', '股票', '房产', '时尚', '体育', '社会', '家居', '游戏', '彩票', '科技', '教育', '时政', '娱乐', '财经']\n",
    "\n",
    "# 每篇文档保留的文档数量\n",
    "#per_class_max_docs = 1000\n",
    "\n",
    "def load_data_to_mini(path, to_path, per_class_max_docs=1000):\n",
    "    \"\"\"\n",
    "    处理清华大学语料库，将类别和文档处理成fasttext 所需要的格式\n",
    "    :param path: \n",
    "    :param to_path: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # 抽取后的语料库\n",
    "    corpus = []\n",
    "    if not os.path.isdir(path):\n",
    "        print('path error')\n",
    "    # 列举当前目录下的所有子列别目录\n",
    "    with codecs.open(to_path, 'w+',encoding='utf-8',errors='ignore') as f:\n",
    "        for files in os.listdir(path):\n",
    "            curr_path = os.path.join(path, files)\n",
    "            print(curr_path)\n",
    "            if os.path.isdir(curr_path):\n",
    "                count = 0\n",
    "                docs = []\n",
    "                for file in os.listdir(curr_path):\n",
    "                    count += 1\n",
    "                    if count > per_class_max_docs:\n",
    "                        break\n",
    "                    file_path = os.path.join(curr_path, file)\n",
    "                    \n",
    "                    # 读取文件中的内容\n",
    "                    with codecs.open(file_path, 'r', encoding='utf-8',errors='ignore') as fd:\n",
    "                        docs.append('__label__' + files + ' ' + ' '.join(jieba.cut(re.sub('[  \\n\\r\\t]+', '', fd.readline()))))\n",
    "                        s = ' '.join(jieba.cut(re.sub('[  \\n\\r\\t]+', '', fd.read())))\n",
    "                        f.write('__label__' + files + ' ' + s+'\\n')\n",
    "            corpus.append(docs)\n",
    "#     # 将数据写到一个新的文件中\n",
    "#     with codecs.open(to_path, 'a+', encoding='utf-8',errors='ignore') as f:\n",
    "#         for docs in corpus:\n",
    "#             for doc in docs:\n",
    "#                 f.write(str(doc)+'\\n')\n",
    "#                 print(doc)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/THUCNews\\体育\n",
      "../data/THUCNews\\娱乐\n",
      "../data/THUCNews\\家居\n",
      "../data/THUCNews\\彩票\n",
      "../data/THUCNews\\房产\n",
      "../data/THUCNews\\教育\n",
      "../data/THUCNews\\时尚\n",
      "../data/THUCNews\\时政\n",
      "../data/THUCNews\\星座\n",
      "../data/THUCNews\\游戏\n",
      "../data/THUCNews\\社会\n",
      "../data/THUCNews\\科技\n",
      "../data/THUCNews\\股票\n",
      "../data/THUCNews\\财经\n"
     ]
    }
   ],
   "source": [
    "corpus = load_data_to_mini('../data/THUCNews', 'thu_data_all.txt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size(14,1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__label__体育 商瑞华 首战 复仇 心切 中国 玫瑰 要 用 美国 方式 攻克 瑞典'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('corpus size(%d,%d)' %(len(corpus), len(corpus[0])))\n",
    "corpus[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_with_label(corpus):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和样本标签\n",
    "    :param corpus: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x = []\n",
    "    input_y = []\n",
    "\n",
    "    tag = []\n",
    "    if os.path.isfile(corpus):\n",
    "        with codecs.open(corpus, 'rb',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tag.append(line)\n",
    "\n",
    "    else:\n",
    "        for docs in corpus:\n",
    "            for doc in docs:\n",
    "                tag.append(doc)\n",
    "    tag = shuffle(tag)\n",
    "    for doc in tag:\n",
    "#         print(doc)\n",
    "        index = doc.find(' ')\n",
    "        input_y.append(doc[:index])\n",
    "        input_x.append(doc[index + 1 :])\n",
    "\n",
    "    # 打乱数据，避免在采样的时候出现类别不均衡现象\n",
    "    # datasets = np.column_stack([input_x, input_y])\n",
    "    # np.random.shuffle(datasets)\n",
    "    # input_x = []\n",
    "    # input_y = []\n",
    "    # for i in datasets:\n",
    "    #     input_x.append(i[:-1])\n",
    "    #     input_y.append(i[-1:])\n",
    "    return [input_x, input_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time\n",
    "\n",
    "def feature_extractor(input_x, case='tfidf', max_df=1.0, min_df=0.0):\n",
    "    \"\"\"\n",
    "    特征抽取\n",
    "    :param corpus: \n",
    "    :param case: 不同的特征抽取方法\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return TfidfVectorizer(token_pattern='\\w', ngram_range=(1,2), max_df=max_df, min_df=min_df).fit_transform(input_x)\n",
    "\n",
    "# 拆分数据集\n",
    "def split_data_to_train_and_test(corpus, indices=0.2, random_state=10, shuffle=True):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和测试数据\n",
    "    :param corpus: [input_x]\n",
    "    :param indices: 划分比例\n",
    "    :random_state: 随机种子\n",
    "    :param shuffle: 是否打乱数据\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x, y = corpus\n",
    "\n",
    "    # 切分数据集\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(input_x, y, test_size=indices, random_state=10)\n",
    "    print(\"Vocabulary Size: {:d}\".format(input_x.shape[1]))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, x_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 917401\n",
      "Train/Dev split: 11200/2801\n",
      "\t\t使用 max_df,min_df=(1.0,0.0) 进行特征选择的逻辑回归文本分类\t\t\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         1\n",
      " __label__体育       1.00      0.99      0.99       211\n",
      " __label__娱乐       0.98      0.98      0.98       205\n",
      " __label__家居       0.88      0.99      0.93       191\n",
      " __label__彩票       0.99      0.99      0.99       188\n",
      " __label__房产       0.94      0.95      0.95       192\n",
      " __label__教育       0.95      0.86      0.90       210\n",
      " __label__时尚       0.98      0.93      0.95       202\n",
      " __label__时政       0.89      0.93      0.91       220\n",
      " __label__星座       0.97      0.98      0.98       187\n",
      " __label__游戏       0.91      0.94      0.92       204\n",
      " __label__社会       0.88      0.89      0.89       228\n",
      " __label__科技       0.93      0.90      0.91       178\n",
      " __label__股票       0.95      0.89      0.91       200\n",
      " __label__财经       0.88      0.89      0.89       184\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      2801\n",
      "   macro avg       0.87      0.87      0.87      2801\n",
      "weighted avg       0.94      0.94      0.94      2801\n",
      "\n",
      "accuracy_score: 0.93538s\n",
      "time uesed: 104.6680s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def fit_and_predicted(train_x, train_y, test_x, test_y, penalty='l2', C=1.0, solver='lbfgs'):\n",
    "    \"\"\"\n",
    "    训练与预测\n",
    "    :param train_x: \n",
    "    :param train_y: \n",
    "    :param test_x: \n",
    "    :param test_y: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    clf = linear_model.LogisticRegression(penalty=penalty, C=C, solver=solver, n_jobs=-1).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "\n",
    "# 1. 加载语料\n",
    "corpus = split_data_with_label('thu_data_all.txt')\n",
    "\n",
    "input_x, y = corpus\n",
    "# 2. 特征选择\n",
    "input_x = feature_extractor(input_x, 'tfidf')\n",
    "# 3.切分训练数据和测试数据\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test([input_x, y])\n",
    "\n",
    "# 4. 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t\\t使用 max_df,min_df=(1.0,0.0) 进行特征选择的逻辑回归文本分类\\t\\t')\n",
    "\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练（加入交叉验证）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 917401\n",
      "Train/Dev split: 11200/2801\n",
      "\t 使用 max_df,min_df=(1.0,0.0) 进行特征选择的逻辑回归文本分类\t\t\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         1\n",
      " __label__体育       1.00      0.99      0.99       211\n",
      " __label__娱乐       0.98      0.98      0.98       205\n",
      " __label__家居       0.88      0.99      0.93       191\n",
      " __label__彩票       0.99      0.99      0.99       188\n",
      " __label__房产       0.94      0.95      0.95       192\n",
      " __label__教育       0.95      0.86      0.90       210\n",
      " __label__时尚       0.98      0.93      0.95       202\n",
      " __label__时政       0.89      0.93      0.91       220\n",
      " __label__星座       0.97      0.98      0.98       187\n",
      " __label__游戏       0.91      0.94      0.92       204\n",
      " __label__社会       0.88      0.89      0.89       228\n",
      " __label__科技       0.93      0.90      0.91       178\n",
      " __label__股票       0.95      0.89      0.91       200\n",
      " __label__财经       0.88      0.89      0.89       184\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      2801\n",
      "   macro avg       0.87      0.87      0.87      2801\n",
      "weighted avg       0.94      0.94      0.94      2801\n",
      "\n",
      "accuracy_score: 0.93538s\n",
      "time uesed: 102.1269s\n"
     ]
    }
   ],
   "source": [
    "def fit_and_predicted_use_CV(train_x, train_y, test_x, test_y, penalty='l2', C=1.0, solver='lbfgs', cv=10):\n",
    "    \"\"\"\n",
    "    训练与预测\n",
    "    :param train_x: \n",
    "    :param train_y: \n",
    "    :param test_x: \n",
    "    :param test_y: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    clf = linear_model.LogisticRegressionCV(penalty=penalty, C=C, solver=solver, n_jobs=-1, cv=cv).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))\n",
    "    \n",
    "\n",
    "\n",
    "input_x, y = corpus\n",
    "input_x = feature_extractor(input_x, 'tfidf')\n",
    "# 切分训练数据和测试数据\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test([input_x, y])\n",
    "# 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t 使用 max_df,min_df=(1.0,0.0) 进行特征选择的逻辑回归文本分类\\t\\t\\n')\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
